{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNTK_setup_IU6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KANZbH_JGtHL","executionInfo":{"status":"ok","timestamp":1622903476493,"user_tz":-480,"elapsed":503,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"6a3b5e25-16df-43c9-cd08-04ec96078e43"},"source":["import sys\n","print(\"Python version\")\n","print (sys.version)\n","print(\"Version info.\")\n","print (sys.version_info)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Python version\n","3.7.10 (default, May  3 2021, 02:48:31) \n","[GCC 7.5.0]\n","Version info.\n","sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7AuGRPNWD43R"},"source":["**Mount Google Drive for persistent storage**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHWpazk_EJRg","executionInfo":{"status":"ok","timestamp":1622903476823,"user_tz":-480,"elapsed":10,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"9937cb6f-f760-4767-a2da-2b17b4d13d64"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0BZ3UztHQKt","executionInfo":{"status":"ok","timestamp":1622903477588,"user_tz":-480,"elapsed":772,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"79da71be-43f8-4ba3-d78b-8f27aa1443d6"},"source":["# try to get the bare minimum to get a new conda env working\n","conda_path = ''\n","try:\n","    conda_path = !which conda\n","finally:\n","    print('')\n","\n","if (len(conda_path) == 0):\n","    print('installing miniconda')\n","    !wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/local\n","    !conda update conda -y -q\n","    !source /usr/local/etc/profile.d/conda.sh\n","    !conda init \n","    !conda install -n root _license -y -q\n","else:\n","    print('found miniconda')\n","\n","conda_envs = !conda env list\n","res = [i for i in conda_envs if 'test36' in i]\n","if (len(res) == 0):\n","    print('not found test36 env', len(res))\n","    !conda create -y -q --name test36 python=3.6\n","else:\n","    print('found test36 env', len(res))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n","found miniconda\n","found test36 env 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3XOjQ5-PbHB9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903477589,"user_tz":-480,"elapsed":6,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"c98df27b-5151-414c-eee1-5b4d57072fbd"},"source":["%%bash\n","sudo ln -s /usr/lib/x86_64-linux-gnu/libmpi_cxx.so.20 /usr/lib/x86_64-linux-gnu/libmpi_cxx.so.1\n","sudo ln -s /usr/lib/x86_64-linux-gnu/libmpi.so.20.10.1 /usr/lib/x86_64-linux-gnu/libmpi.so.12\n","export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/x86_64-linux-gnu"],"execution_count":4,"outputs":[{"output_type":"stream","text":["ln: failed to create symbolic link '/usr/lib/x86_64-linux-gnu/libmpi_cxx.so.1': File exists\n","ln: failed to create symbolic link '/usr/lib/x86_64-linux-gnu/libmpi.so.12': File exists\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Vu-Z1ipNJ3bC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903482275,"user_tz":-480,"elapsed":4690,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"28bab417-f966-4f31-f9e7-8552f2cd65a6"},"source":["%%bash\n","source activate test36\n","\n","#python3 -m pip install cntk\n","python3 -m pip install cntk-gpu\n","python3 -m pip install google.colab\n","python3 -m pip install matplotlib\n","ipython kernel install --user --name=test36\n","\n","python3\n","import sys\n","# maybe only need this the first time we run this notebook\n","#sys.path.append('/usr/local/lib/python3.6/site-packages')\n","\n","import cntk as C\n","print(C.device.gpu(0))\n","print(\"Python version\")\n","print(sys.version)\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cntk-gpu in /usr/local/envs/test36/lib/python3.6/site-packages (2.7)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/envs/test36/lib/python3.6/site-packages (from cntk-gpu) (1.19.5)\n","Requirement already satisfied: scipy>=0.17 in /usr/local/envs/test36/lib/python3.6/site-packages (from cntk-gpu) (1.5.4)\n","Requirement already satisfied: google.colab in /usr/local/envs/test36/lib/python3.6/site-packages (1.0.0)\n","Collecting ipykernel~=4.6.0\n","  Using cached ipykernel-4.6.1-py3-none-any.whl (104 kB)\n","Requirement already satisfied: requests~=2.21.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (2.21.0)\n","Requirement already satisfied: pandas~=0.24.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (0.24.2)\n","Requirement already satisfied: tornado~=4.5.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (4.5.3)\n","Requirement already satisfied: six~=1.12.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (1.12.0)\n","Requirement already satisfied: notebook~=5.2.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (5.2.2)\n","Requirement already satisfied: ipython~=5.5.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (5.5.0)\n","Requirement already satisfied: portpicker~=1.2.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (1.2.0)\n","Requirement already satisfied: google-auth~=1.4.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google.colab) (1.4.2)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from google-auth~=1.4.0->google.colab) (4.2.2)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/envs/test36/lib/python3.6/site-packages (from google-auth~=1.4.0->google.colab) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from google-auth~=1.4.0->google.colab) (0.2.8)\n","Requirement already satisfied: jupyter-client in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel~=4.6.0->google.colab) (6.1.12)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel~=4.6.0->google.colab) (4.3.3)\n","Requirement already satisfied: pexpect in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (5.0.9)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (1.0.18)\n","Requirement already satisfied: pickleshare in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (2.9.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (0.8.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython~=5.5.0->google.colab) (52.0.0.post20210125)\n","Requirement already satisfied: terminado>=0.3.3 in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (0.10.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (0.2.0)\n","Requirement already satisfied: jinja2 in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (3.0.1)\n","Requirement already satisfied: nbconvert in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (6.0.7)\n","Requirement already satisfied: jupyter-core in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (4.7.1)\n","Requirement already satisfied: nbformat in /usr/local/envs/test36/lib/python3.6/site-packages (from notebook~=5.2.0->google.colab) (5.1.3)\n","Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from pandas~=0.24.0->google.colab) (2.8.1)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from pandas~=0.24.0->google.colab) (1.19.5)\n","Requirement already satisfied: pytz>=2011k in /usr/local/envs/test36/lib/python3.6/site-packages (from pandas~=0.24.0->google.colab) (2021.1)\n","Requirement already satisfied: wcwidth in /usr/local/envs/test36/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google.colab) (0.2.5)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/envs/test36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google.colab) (0.4.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/test36/lib/python3.6/site-packages (from requests~=2.21.0->google.colab) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from requests~=2.21.0->google.colab) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/envs/test36/lib/python3.6/site-packages (from requests~=2.21.0->google.colab) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/test36/lib/python3.6/site-packages (from requests~=2.21.0->google.colab) (2021.5.30)\n","Requirement already satisfied: ptyprocess in /usr/local/envs/test36/lib/python3.6/site-packages (from terminado>=0.3.3->notebook~=5.2.0->google.colab) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from jinja2->notebook~=5.2.0->google.colab) (2.0.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/envs/test36/lib/python3.6/site-packages (from jupyter-client->ipykernel~=4.6.0->google.colab) (22.1.0)\n","Requirement already satisfied: testpath in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.7.1)\n","Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.5.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (1.4.3)\n","Requirement already satisfied: bleach in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (3.3.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.3)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/envs/test36/lib/python3.6/site-packages (from nbconvert->notebook~=5.2.0->google.colab) (0.1.2)\n","Requirement already satisfied: async-generator in /usr/local/envs/test36/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google.colab) (1.10)\n","Requirement already satisfied: nest-asyncio in /usr/local/envs/test36/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook~=5.2.0->google.colab) (1.5.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/envs/test36/lib/python3.6/site-packages (from nbformat->notebook~=5.2.0->google.colab) (3.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/envs/test36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (4.5.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (21.2.0)\n","Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (0.17.3)\n","Requirement already satisfied: webencodings in /usr/local/envs/test36/lib/python3.6/site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/envs/test36/lib/python3.6/site-packages (from bleach->nbconvert->notebook~=5.2.0->google.colab) (20.9)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/envs/test36/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (3.10.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/envs/test36/lib/python3.6/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google.colab) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/envs/test36/lib/python3.6/site-packages (from packaging->bleach->nbconvert->notebook~=5.2.0->google.colab) (2.4.7)\n","Installing collected packages: ipykernel\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 5.5.5\n","    Uninstalling ipykernel-5.5.5:\n","      Successfully uninstalled ipykernel-5.5.5\n","Successfully installed ipykernel-4.6.1\n","Requirement already satisfied: matplotlib in /usr/local/envs/test36/lib/python3.6/site-packages (3.3.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (1.19.5)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from matplotlib) (8.2.0)\n","Requirement already satisfied: six in /usr/local/envs/test36/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n","Installed kernelspec test36 in /root/.local/share/jupyter/kernels/test36\n","GPU[0] Tesla T4\n","Python version\n","3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n","[GCC 7.3.0]\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n","WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n","WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n","/usr/local/envs/test36/lib/python3.6/site-packages/cntk/cntk_py_init.py:56: UserWarning: Unsupported Linux distribution (ubuntu-18.04). CNTK supports Ubuntu 16.04 and above, only.\n","  warnings.warn('Unsupported Linux distribution (%s-%s). CNTK supports Ubuntu 16.04 and above, only.' % (__my_distro__, __my_distro_ver__))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oMNqBdQoftLr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903483608,"user_tz":-480,"elapsed":1344,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"1125fd93-5770-469d-a809-ecbdc2e61b70"},"source":["%%bash\n","source activate test36\n","pip install --upgrade ipykernel\n","jupyter kernelspec list"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ipykernel in /usr/local/envs/test36/lib/python3.6/site-packages (4.6.1)\n","Collecting ipykernel\n","  Using cached ipykernel-5.5.5-py3-none-any.whl (120 kB)\n","Requirement already satisfied: jupyter-client in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel) (6.1.12)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel) (5.5.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel) (4.5.3)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipykernel) (4.3.3)\n","Requirement already satisfied: decorator in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (5.0.9)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (1.0.18)\n","Requirement already satisfied: pickleshare in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (0.8.1)\n","Requirement already satisfied: pexpect in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (2.9.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/envs/test36/lib/python3.6/site-packages (from ipython>=5.0.0->ipykernel) (52.0.0.post20210125)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel) (1.12.0)\n","Requirement already satisfied: wcwidth in /usr/local/envs/test36/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel) (0.2.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/envs/test36/lib/python3.6/site-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/envs/test36/lib/python3.6/site-packages (from jupyter-client->ipykernel) (4.7.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/envs/test36/lib/python3.6/site-packages (from jupyter-client->ipykernel) (2.8.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/envs/test36/lib/python3.6/site-packages (from jupyter-client->ipykernel) (22.1.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/envs/test36/lib/python3.6/site-packages (from pexpect->ipython>=5.0.0->ipykernel) (0.7.0)\n","Installing collected packages: ipykernel\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.6.1\n","    Uninstalling ipykernel-4.6.1:\n","      Successfully uninstalled ipykernel-4.6.1\n","Successfully installed ipykernel-5.5.5\n","Available kernels:\n","  test36     /root/.local/share/jupyter/kernels/test36\n","  python3    /usr/local/envs/test36/share/jupyter/kernels/python3\n","  ir         /usr/local/share/jupyter/kernels/ir\n","  python2    /usr/local/share/jupyter/kernels/python2\n"],"name":"stdout"},{"output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires ipykernel~=4.6.0, but you have ipykernel 5.5.5 which is incompatible.\n","WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EEOiEJkIMQMO"},"source":["**Get notebook from persistent location and copy to local for faster processing and temp data creation**"]},{"cell_type":"code","metadata":{"id":"HYzu97F8EkFg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903484099,"user_tz":-480,"elapsed":496,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"87dc62a0-9096-452a-e90f-29f108fe59f2"},"source":["!rm /content/IU6*\n","#!cp  /content/gdrive/MyDrive/'Colab Notebooks'/DLF_Lab2_LogisticRegression* /content\n","!cp  /content/gdrive/MyDrive/PAI0221A/4.DLF/IU6* /content\n","!ls -la /content/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["total 57148\n","drwxr-xr-x 1 root root     4096 Jun  5 14:31 .\n","drwxr-xr-x 1 root root     4096 Jun  5 14:15 ..\n","drwxr-xr-x 4 root root     4096 Jun  1 13:40 .config\n","drwx------ 5 root root     4096 Jun  5 14:16 gdrive\n","-rw------- 1 root root    26498 Jun  5 14:31 IU6_Lab3_MultiLayerPerceptron.ipynb\n","-rw-r--r-- 1 root root 58468498 Jun  7  2018 Miniconda3-4.5.4-Linux-x86_64.sh\n","drwxr-xr-x 1 root root     4096 Jun  1 13:40 sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A9tMexJXKUMB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903498354,"user_tz":-480,"elapsed":14258,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"afc0259f-bb56-4de0-c58c-92dba0ec3444"},"source":["!date\n","!jupyter nbconvert --to notebook --ExecutePreprocessor.kernel_name=test36 --ExecutePreprocessor.timeout=3000 --execute /content/IU6_Lab3_MultiLayerPerceptron.ipynb --output /content/DLF_IU6_Lab3_MultiLayerPerceptron_output.ipynb\n","!date"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Sat Jun  5 14:31:24 UTC 2021\n","[NbConvertApp] Converting notebook /content/IU6_Lab3_MultiLayerPerceptron.ipynb to notebook\n","/usr/local/envs/test36/lib/python3.6/site-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n","  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n","[NbConvertApp] Executing notebook with kernel: test36\n","Selected GPU[0] Tesla T4 as the process wide default device.\n","[NbConvertApp] Writing 58467 bytes to /content/DLF_IU6_Lab3_MultiLayerPerceptron_output.ipynb\n","Sat Jun  5 14:31:38 UTC 2021\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ww0avubm7JPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903498354,"user_tz":-480,"elapsed":17,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"52775057-24f9-49e7-80e9-932f3689ca2e"},"source":["!ls './gdrive/MyDrive/PAI0221A/4.DLF/'"],"execution_count":9,"outputs":[{"output_type":"stream","text":["data\n","DLF_IU12_Lab6_TextClassification_with_LSTM_output.ipynb\n","DLF_IU5_Lab2_LogisticRegression_output.ipynb\n","DLF_IU6_Lab3_MultiLayerPerceptron_output.ipynb\n","DLF_IU8_Lab4_ConvolutionalNeuralNetwork_output1.ipynb\n","DLF_IU8_Lab4_ConvolutionalNeuralNetwork_output.ipynb\n","DLF_Lab1_MNIST_DataLoader_output.ipynb\n","IU10_LSTM_TS.ipynb\n","IU12_Lab6_TextClassification_with_LSTM.ipynb\n","IU4_Assignment.py\n","IU5_Lab2_LogisticRegression.ipynb\n","IU6_Lab3_MultiLayerPerceptron.ipynb\n","IU8_Lab4_ConvolutionalNeuralNetwork.ipynb\n","IU8_MysteryNumberD.bmp\n","Lab1_MNIST_DataLoader.ipynb\n","MysteryNumberD.bmp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SqtC5OyL-7CL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622903499284,"user_tz":-480,"elapsed":943,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}},"outputId":"415b0b69-b233-433b-a94b-034d7dbdb76d"},"source":["!cat /content/DLF_IU6_Lab3_MultiLayerPerceptron_output.ipynb"],"execution_count":10,"outputs":[{"output_type":"stream","text":["{\n"," \"cells\": [\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 1,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"06GYe6MniIRx\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"from IPython.display import Image as Image1\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"hd1Sw7SDiIR0\",\n","    \"nbpresent\": {\n","     \"id\": \"29b9bd1d-766f-4422-ad96-de0accc1ce58\"\n","    }\n","   },\n","   \"source\": [\n","    \"# Lab 3 - Multi Layer Perceptron with MNIST\\n\",\n","    \"\\n\",\n","    \"This lab corresponds to Module 3 of the \\\"Deep Learning Explained\\\" course.  We assume that you have successfully completed Lab 1 (Downloading the MNIST data).\\n\",\n","    \"\\n\",\n","    \"In this lab, we train a multi-layer perceptron on MNIST data. This notebook provides the recipe using Python APIs. \\n\",\n","    \"\\n\",\n","    \"## Introduction\\n\",\n","    \"\\n\",\n","    \"**Problem** \\n\",\n","    \"We will continue to work on the same problem of recognizing digits in MNIST data. The MNIST data comprises of hand-written digits with little background noise.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 2,\n","   \"metadata\": {\n","    \"colab\": {\n","     \"base_uri\": \"https://localhost:8080/\",\n","     \"height\": 221\n","    },\n","    \"executionInfo\": {\n","     \"elapsed\": 290,\n","     \"status\": \"ok\",\n","     \"timestamp\": 1622438237496,\n","     \"user\": {\n","      \"displayName\": \"Rajesh kumar Ganesan\",\n","      \"photoUrl\": \"\",\n","      \"userId\": \"04176200856727597046\"\n","     },\n","     \"user_tz\": -480\n","    },\n","    \"id\": \"0Sp20QdTiIR1\",\n","    \"outputId\": \"a41b2ae2-87e7-4279-e074-bdd0e4f3fc1d\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"data\": {\n","      \"text/html\": [\n","       \"<img src=\\\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\\\" width=\\\"200\\\" height=\\\"200\\\"/>\"\n","      ],\n","      \"text/plain\": [\n","       \"<IPython.core.display.Image object>\"\n","      ]\n","     },\n","     \"execution_count\": 2,\n","     \"metadata\": {},\n","     \"output_type\": \"execute_result\"\n","    }\n","   ],\n","   \"source\": [\n","    \"# Figure 1\\n\",\n","    \"Image1(url= \\\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s1600/mnist_originals.png\\\", width=200, height=200)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"LmfWKYUHiIR3\"\n","   },\n","   \"source\": [\n","    \"**Goal**:\\n\",\n","    \"Our goal is to train a classifier that will identify the digits in the MNIST dataset. Additionally, we aspire to achieve lower error rate with Multi-layer perceptron compared to Multi-class logistic regression. \\n\",\n","    \"\\n\",\n","    \"**Approach**:\\n\",\n","    \"There are 4 stages in this lab: \\n\",\n","    \"- **Data reading**: We will use the CNTK Text reader.  \\n\",\n","    \"- **Data preprocessing**: Covered in part A (suggested extension section). \\n\",\n","    \"- **Model creation**: Multi-Layer Perceptron model.\\n\",\n","    \"- **Train-Test-Predict**: This is the same workflow introduced in the lectures\\n\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 3,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"hm8iWHwdiIR3\",\n","    \"nbpresent\": {\n","     \"id\": \"138d1a78-02e2-4bd6-a20e-07b83f303563\"\n","    }\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stderr\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"/usr/local/envs/test36/lib/python3.6/site-packages/cntk/cntk_py_init.py:56: UserWarning: Unsupported Linux distribution (ubuntu-18.04). CNTK supports Ubuntu 16.04 and above, only.\\n\",\n","      \"  warnings.warn('Unsupported Linux distribution (%s-%s). CNTK supports Ubuntu 16.04 and above, only.' % (__my_distro__, __my_distro_ver__))\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\\n\",\n","    \"import matplotlib.image as mpimg\\n\",\n","    \"import matplotlib.pyplot as plt\\n\",\n","    \"import numpy as np\\n\",\n","    \"import sys\\n\",\n","    \"import os\\n\",\n","    \"\\n\",\n","    \"import cntk as C\\n\",\n","    \"\\n\",\n","    \"%matplotlib inline\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"DSmtoV6qiIR4\"\n","   },\n","   \"source\": [\n","    \"In the block below, we check if we are running this notebook in the CNTK internal test machines by looking for environment variables defined there. We then select the right target device (GPU vs CPU) to test this notebook. In other cases, we use CNTK's default policy to use the best available device (GPU, if available, else CPU).\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 4,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"-DBiGIlkiIR4\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Select the right target device when this notebook is being tested:\\n\",\n","    \"if 'TEST_DEVICE' in os.environ:\\n\",\n","    \"    if os.environ['TEST_DEVICE'] == 'cpu':\\n\",\n","    \"        C.device.try_set_default_device(C.device.cpu())\\n\",\n","    \"    else:\\n\",\n","    \"        C.device.try_set_default_device(C.device.gpu(0))\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 5,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"FbYxiIf_iIR5\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"C_Version:2.7\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"# Test for CNTK version\\n\",\n","    \"print('C_Version:'+C.__version__)\\n\",\n","    \"if not C.__version__ == \\\"2.7\\\":\\n\",\n","    \"    raise Exception(\\\"this lab is designed to work with 2.0. Current Version: \\\" + C.__version__) \"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 6,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"ZAuKcTfBiIR5\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Ensure we always get the same amount of randomness\\n\",\n","    \"np.random.seed(0)\\n\",\n","    \"C.cntk_py.set_fixed_random_seed(1)\\n\",\n","    \"C.cntk_py.force_deterministic_algorithms()\\n\",\n","    \"\\n\",\n","    \"# Define the data dimensions\\n\",\n","    \"input_dim = 784\\n\",\n","    \"num_output_classes = 10\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"JQ-CBbqliIR5\"\n","   },\n","   \"source\": [\n","    \"## Data reading\\n\",\n","    \"\\n\",\n","    \"There are different ways one can read data into CNTK. The easiest way is to load the data in memory using NumPy / SciPy / Pandas readers. However, this can be done only for small data sets. Since deep learning requires large amount of data we have chosen in this course to show how to leverage built-in distributed readers that can scale to terrabytes of data with little extra effort. \\n\",\n","    \"\\n\",\n","    \"We are using the MNIST data you have downloaded using Lab 1 DataLoader notebook. The dataset has 60,000 training images and 10,000 test images with each image being 28 x 28 pixels. Thus the number of features is equal to 784 (= 28 x 28 pixels), 1 per pixel. The variable `num_output_classes` is set to 10 corresponding to the number of digits (0-9) in the dataset.\\n\",\n","    \"\\n\",\n","    \"In Lab 1, the data was downloaded and written to 2 CTF (CNTK Text Format) files, 1 for training, and 1 for testing. Each line of these text files takes the form:\\n\",\n","    \"\\n\",\n","    \"    |labels 0 0 0 1 0 0 0 0 0 0 |features 0 0 0 0 ... \\n\",\n","    \"                                                  (784 integers each representing a pixel)\\n\",\n","    \"    \\n\",\n","    \"We are going to use the image pixels corresponding the integer stream named \\\"features\\\". We define a `create_reader` function to read the training and test data using the [CTF deserializer](https://cntk.ai/pythondocs/cntk.io.html?highlight=ctfdeserializer#cntk.io.CTFDeserializer). The labels are [1-hot encoded](https://en.wikipedia.org/wiki/One-hot). Refer to Lab 1 for data format visualizations. \"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 7,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"yZQ9yaJPiIR6\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\\n\",\n","    \"def create_reader(path, is_training, input_dim, num_label_classes):\\n\",\n","    \"    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\\n\",\n","    \"        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\\n\",\n","    \"        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\\n\",\n","    \"    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 8,\n","   \"metadata\": {\n","    \"id\": \"rx_oU90miIR7\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"Data directory is data/MNIST\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"# Ensure the training and test data is generated and available for this tutorial.\\n\",\n","    \"# We search in two locations in the toolkit for the cached MNIST data set.\\n\",\n","    \"data_found = False\\n\",\n","    \"os.chdir('./gdrive/MyDrive/PAI0221A/4.DLF/')\\n\",\n","    \"for data_dir in [os.path.join(\\\"..\\\", \\\"Examples\\\", \\\"Image\\\", \\\"DataSets\\\", \\\"MNIST\\\"),\\n\",\n","    \"                 os.path.join(\\\"data\\\", \\\"MNIST\\\")]:\\n\",\n","    \"    train_file = os.path.join(data_dir, \\\"Train-28x28_cntk_text.txt\\\")\\n\",\n","    \"    test_file = os.path.join(data_dir, \\\"Test-28x28_cntk_text.txt\\\")\\n\",\n","    \"    if os.path.isfile(train_file) and os.path.isfile(test_file):\\n\",\n","    \"        data_found = True\\n\",\n","    \"        break\\n\",\n","    \"if not data_found:\\n\",\n","    \"    raise ValueError(\\\"Please generate the data by completing Lab1_MNIST_DataLoader\\\")\\n\",\n","    \"print(\\\"Data directory is {0}\\\".format(data_dir))\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"OpNz6hn8iIR8\"\n","   },\n","   \"source\": [\n","    \"<a id='#Model Creation'></a>\\n\",\n","    \"## Model Creation\\n\",\n","    \"\\n\",\n","    \"Our multi-layer perceptron will be relatively simple with 2 hidden layers (`num_hidden_layers`). The number of nodes in the hidden layer being a parameter specified by `hidden_layers_dim`. The figure below illustrates the entire model we will use in this tutorial in the context of MNIST data.\\n\",\n","    \"\\n\",\n","    \"![model-mlp](http://cntk.ai/jup/cntk103c_MNIST_MLP.png)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"jKtERuiviIR8\"\n","   },\n","   \"source\": [\n","    \"If you are not familiar with the terms *hidden_layer* and *number of hidden layers*, please review the module 3 course videos.\\n\",\n","    \"\\n\",\n","    \"Each Dense layer (as illustrated below) shows the input dimensions, output dimensions and activation function that layer uses. Specifically, the layer below shows: input dimension = 784 (1 dimension for each input pixel), output dimension = 400 (number of hidden nodes, a parameter specified by the user) and activation function being [relu](https://cntk.ai/pythondocs/cntk.ops.html?highlight=relu#cntk.ops.relu).\\n\",\n","    \"\\n\",\n","    \"![model-dense](http://www.cntk.ai/jup/cntk103c_MNIST_dense.png)\\n\",\n","    \"\\n\",\n","    \"In this model we have 2 dense layer called the hidden layers each with an activation function of `relu`.  These are followed by the dense output layer with no activation.  \\n\",\n","    \"\\n\",\n","    \"The output dimension (a.k.a. number of hidden nodes) in the 2 hidden layer is set to 400. The number of hidden layers is 2. \\n\",\n","    \"\\n\",\n","    \"The final output layer emits a vector of 10 values. Since we will be using softmax to normalize the output of the model we do not use an activation function in this layer. The softmax operation comes bundled with the [loss function](https://cntk.ai/pythondocs/cntk.losses.html) we will be using later in this tutorial.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 9,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"BjOJFBtQiIR9\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"num_hidden_layers = 2\\n\",\n","    \"hidden_layers_dim = 400\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"BOC43nfPiIR9\"\n","   },\n","   \"source\": [\n","    \"Network input and output: \\n\",\n","    \"- **input** variable (a key CNTK concept): \\n\",\n","    \">An **input** variable is a container in which we fill different observations in this case image pixels during model learning (a.k.a.training) and model evaluation (a.k.a. testing). Thus, the shape of the `input` must match the shape of the data that will be provided.  For example, when data are images each of  height 10 pixels  and width 5 pixels, the input feature dimension will be 50 (representing the total number of image pixels). More on data and their dimensions to appear in separate tutorials.\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"**Knowledge Check** What is the input dimension of your chosen model? This is fundamental to our understanding of variables in a network or model representation in CNTK.\\n\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 10,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"m8wxU3L3iIR9\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"input = C.input_variable(input_dim)\\n\",\n","    \"label = C.input_variable(num_output_classes)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"Yr6bDm09iIR-\"\n","   },\n","   \"source\": [\n","    \"## Multi-layer Perceptron setup\\n\",\n","    \"\\n\",\n","    \"The code below is a direct translation of the model shown above.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 11,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"Gk5a3Y2PiIR-\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"def create_model(features):\\n\",\n","    \"    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\\n\",\n","    \"            h = features\\n\",\n","    \"            for _ in range(num_hidden_layers):\\n\",\n","    \"                h = C.layers.Dense(hidden_layers_dim)(h)\\n\",\n","    \"            r = C.layers.Dense(num_output_classes, activation = None)(h)\\n\",\n","    \"            return r\\n\",\n","    \"        \\n\",\n","    \"z = create_model(input)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"TMjShAK5iIR-\"\n","   },\n","   \"source\": [\n","    \"`z` will be used to represent the output of a network.\\n\",\n","    \"\\n\",\n","    \"We introduced sigmoid function in CNTK 102, in this tutorial you should try different activation functions in the hidden layer. You may choose to do this right away and take a peek into the performance later in the tutorial or run the preset tutorial and then choose to perform the suggested exploration.\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"** Suggested Exploration **\\n\",\n","    \"- Record the training error you get with `sigmoid` as the activation function\\n\",\n","    \"- Now change to `relu` as the activation function and see if you can improve your training error\\n\",\n","    \"\\n\",\n","    \"**Knowledge Check**: Name some of the different supported activation functions.  Which activation function gives the least training error?\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 12,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"YKJi16wRiIR_\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Scale the input to 0-1 range by dividing each pixel by 255.\\n\",\n","    \"z = create_model(input/255.0)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"fGB-FpLliIR_\"\n","   },\n","   \"source\": [\n","    \"## Training\\n\",\n","    \"â\\n\",\n","    \"Below, we define the **Loss** function, which is used to guide weight changes during training.  \\n\",\n","    \"â\\n\",\n","    \"As explained in the lectures, we use the `softmax` function to map the accumulated evidences or activations to a probability distribution over the classes (Details of the [softmax function][] and other [activation][] functions).\\n\",\n","    \"â\\n\",\n","    \"[softmax function]: http://cntk.ai/pythondocs/cntk.ops.html#cntk.ops.softmax\\n\",\n","    \"[activation]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions\\n\",\n","    \"We minimize the cross-entropy between the label and predicted probability by the network.\\n\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 13,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"nkYtgaeBiIR_\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"loss = C.cross_entropy_with_softmax(z, label)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"405mrGKmiISA\"\n","   },\n","   \"source\": [\n","    \"#### Evaluation\\n\",\n","    \"\\n\",\n","    \"Below, we define the **Evaluation** (or metric) function that is used to report a measurement of how well our model is performing.\\n\",\n","    \"\\n\",\n","    \"For this problem, we choose the **classification_error()** function as our metric, which returns the average error over the associated samples (treating a match as \\\"1\\\", where the model's prediction matches the \\\"ground truth\\\" label, and a non-match as \\\"0\\\").\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 14,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"jLfbuKqoiISA\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"label_error = C.classification_error(z, label)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"ZqY2YaF4iISA\"\n","   },\n","   \"source\": [\n","    \"### Configure training\\n\",\n","    \"\\n\",\n","    \"The trainer strives to reduce the `loss` function by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being a basic one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer would calculate the `loss` or error between the predicted label against the corresponding ground-truth label and using [gradient-decent][] generate a new set model parameters in a single iteration. \\n\",\n","    \"\\n\",\n","    \"The aforementioned model parameter update using a single observation at a time is attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to load a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\\n\",\n","    \"\\n\",\n","    \"With minibatches we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\\n\",\n","    \"\\n\",\n","    \"One of the key parameter for optimization is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. We will be covering more details in later tutorial. \\n\",\n","    \"With this information, we are ready to create our trainer. \\n\",\n","    \"\\n\",\n","    \"[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\\n\",\n","    \"[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\\n\",\n","    \"[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 15,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"-gawwoMCiISB\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Instantiate the trainer object to drive the model training\\n\",\n","    \"learning_rate = 0.2\\n\",\n","    \"lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\\n\",\n","    \"learner = C.sgd(z.parameters, lr_schedule)\\n\",\n","    \"trainer = C.Trainer(z, (loss, label_error), [learner])\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"y4Ouv4lqiISB\"\n","   },\n","   \"source\": [\n","    \"First let us create some helper functions that will be needed to visualize different functions associated with training.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 16,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"xWYZWxpwiISB\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Define a utility function to compute the moving average sum.\\n\",\n","    \"# A more efficient implementation is possible with np.cumsum() function\\n\",\n","    \"def moving_average(a, w=5):\\n\",\n","    \"    if len(a) < w:\\n\",\n","    \"        return a[:]    # Need to send a copy of the array\\n\",\n","    \"    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"# Defines a utility that prints the training progress\\n\",\n","    \"def print_training_progress(trainer, mb, frequency, verbose=1):\\n\",\n","    \"    training_loss = \\\"NA\\\"\\n\",\n","    \"    eval_error = \\\"NA\\\"\\n\",\n","    \"\\n\",\n","    \"    if mb%frequency == 0:\\n\",\n","    \"        training_loss = trainer.previous_minibatch_loss_average\\n\",\n","    \"        eval_error = trainer.previous_minibatch_evaluation_average\\n\",\n","    \"        if verbose: \\n\",\n","    \"            print (\\\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\\\".format(mb, training_loss, eval_error*100))\\n\",\n","    \"        \\n\",\n","    \"    return mb, training_loss, eval_error\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"bg2TlaUAiISC\"\n","   },\n","   \"source\": [\n","    \"<a id='#Run the trainer'></a>\\n\",\n","    \"### Run the trainer\\n\",\n","    \"\\n\",\n","    \"We are now ready to train our fully connected neural net. We want to decide what data we need to feed into the training engine.\\n\",\n","    \"\\n\",\n","    \"In this example, each iteration of the optimizer will work on `minibatch_size` sized samples. We would like to train on all 60000 observations. Additionally we will make multiple passes through the data specified by the variable `num_sweeps_to_train_with`. With these parameters we can proceed with training our simple multi-layer perceptron network.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 17,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"lhRApZ4GiISC\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Initialize the parameters for the trainer\\n\",\n","    \"minibatch_size = 64\\n\",\n","    \"num_samples_per_sweep = 60000\\n\",\n","    \"num_sweeps_to_train_with = 10\\n\",\n","    \"num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 18,\n","   \"metadata\": {\n","    \"id\": \"UCl4th7ViISC\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"Minibatch: 0, Loss: 2.2945, Error: 92.19%\\n\",\n","      \"Minibatch: 500, Loss: 0.2563, Error: 10.94%\\n\",\n","      \"Minibatch: 1000, Loss: 0.0790, Error: 1.56%\\n\",\n","      \"Minibatch: 1500, Loss: 0.1348, Error: 4.69%\\n\",\n","      \"Minibatch: 2000, Loss: 0.0143, Error: 0.00%\\n\",\n","      \"Minibatch: 2500, Loss: 0.0213, Error: 1.56%\\n\",\n","      \"Minibatch: 3000, Loss: 0.0087, Error: 0.00%\\n\",\n","      \"Minibatch: 3500, Loss: 0.0434, Error: 1.56%\\n\",\n","      \"Minibatch: 4000, Loss: 0.0212, Error: 0.00%\\n\",\n","      \"Minibatch: 4500, Loss: 0.0198, Error: 1.56%\\n\",\n","      \"Minibatch: 5000, Loss: 0.0064, Error: 0.00%\\n\",\n","      \"Minibatch: 5500, Loss: 0.0021, Error: 0.00%\\n\",\n","      \"Minibatch: 6000, Loss: 0.0074, Error: 0.00%\\n\",\n","      \"Minibatch: 6500, Loss: 0.0196, Error: 1.56%\\n\",\n","      \"Minibatch: 7000, Loss: 0.0092, Error: 0.00%\\n\",\n","      \"Minibatch: 7500, Loss: 0.0181, Error: 0.00%\\n\",\n","      \"Minibatch: 8000, Loss: 0.0036, Error: 0.00%\\n\",\n","      \"Minibatch: 8500, Loss: 0.0038, Error: 0.00%\\n\",\n","      \"Minibatch: 9000, Loss: 0.0017, Error: 0.00%\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"# Create the reader to training data set\\n\",\n","    \"reader_train = create_reader(train_file, True, input_dim, num_output_classes)\\n\",\n","    \"\\n\",\n","    \"# Map the data streams to the input and labels.\\n\",\n","    \"input_map = {\\n\",\n","    \"    label  : reader_train.streams.labels,\\n\",\n","    \"    input  : reader_train.streams.features\\n\",\n","    \"} \\n\",\n","    \"\\n\",\n","    \"# Run the trainer on and perform model training\\n\",\n","    \"training_progress_output_freq = 500\\n\",\n","    \"\\n\",\n","    \"plotdata = {\\\"batchsize\\\":[], \\\"loss\\\":[], \\\"error\\\":[]}\\n\",\n","    \"\\n\",\n","    \"for i in range(0, int(num_minibatches_to_train)):\\n\",\n","    \"    \\n\",\n","    \"    # Read a mini batch from the training data file\\n\",\n","    \"    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\\n\",\n","    \"    \\n\",\n","    \"    trainer.train_minibatch(data)\\n\",\n","    \"    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\\n\",\n","    \"    \\n\",\n","    \"    if not (loss == \\\"NA\\\" or error ==\\\"NA\\\"):\\n\",\n","    \"        plotdata[\\\"batchsize\\\"].append(batchsize)\\n\",\n","    \"        plotdata[\\\"loss\\\"].append(loss)\\n\",\n","    \"        plotdata[\\\"error\\\"].append(error)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"IsRdqjbwiISC\"\n","   },\n","   \"source\": [\n","    \"Let us plot the errors over the different training minibatches. Note that as we iterate the training loss decreases though we do see some intermediate bumps. \"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 19,\n","   \"metadata\": {\n","    \"id\": \"nkZzl473iISD\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"data\": {\n","      \"image/png\": \"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\\n\",\n","      \"text/plain\": [\n","       \"<Figure size 432x288 with 1 Axes>\"\n","      ]\n","     },\n","     \"metadata\": {\n","      \"needs_background\": \"light\"\n","     },\n","     \"output_type\": \"display_data\"\n","    },\n","    {\n","     \"data\": {\n","      \"image/png\": \"iVBORw0KGgoAAAANSUhEUgAAAYIAAACgCAYAAAAB6WsAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiuklEQVR4nO2de7hVVbn/P1+QmwqCoqYIIunJWyiIbE0L71eSvGSkJZL3jpp6OkfJ7KhdTlfzaJ6fmWklpuYlsyQV71YqF1MI0UTAQCgRVBAQwf3+/njHcs+92Je1N3td9lrv53nmM8ccY8wx3zHWXPMd13fIzAiCIAhqly7lFiAIgiAoL6EIgiAIapxQBEEQBDVOKIIgCIIaJxRBEARBjROKIAiCoMYJRVDhSLpe0mVtjSvpAEkLiyvdh8+dL+mQUjyrsyHJJO1Y6ns3BEmXS5qY3IMkvSupazvS+ZqkGztewqCjCUVQJtLH831J/fP8/5o+AIMBzOxsM/tmIWm2JW4rsg1OMmy0oWlVA5VYHpIel/Re+ki/KekeSdt09HPM7B9mtqmZfdCKPOtVPMzsO2Z2ekfLJOlUSR+kvGePbTv6WbVCKILyMg/4fO5C0seBjcsnTumppI9rJ+RcM9sU+DegL/Dj/AhVXL5PJwWVPRblR2oq/20tkyouww8JRVBebgFOyVyPA36VjSDpF5K+ldwHSFoo6T8kvSFpsaTxTcXN+H0t1RjnSzo54390an0sl7RA0uWZ255M57dTTWvfdM8ZkmZLWiHpRUnDM/fsKWmGpHck3SGpZ1MZTrW5P0v6saSlwOXZrogUp1ENPNV+v5nuWyHpofyWVObe2ZJGZ643krRE0nBJPSVNlLRU0tuSpkrauql0CkXSSElPp/QWS/qJpO550Y6SNDf9Dj+Q1CVz/5eSzG9JelDS9m2VwcyWAXcDu6c050u6WNIMYGUqg30k/SXJ+YKkAzIy7CDpiVS2k4H+mbD832JzSTdLWpRkvlfSJsAfgW2ztfMmftdjJM1KMjwuaZdM2HxJXy3kHWqNJvK/Y8rDaZL+ATwqqYukr0t6Lf2XfiVps7w8fxi/PXJ0JkIRlJdngD6SdpH3wY4FJrZyz0eAzYABwGnAdZL6tRC3f4o7DrhB0sdS2EpcCfUFjgbOkfSZFPapdO6balpPS/oscHm6pw9wDLA086wTgSOAHYChwKkt5KEOmAtsDXy7lfzmOAkYD2wFdAe+2ky828i0soDDgTfN7Dm8DDYDBgJbAGcDqwt8fnN8AFyIl/O+wMHAl/PiHAuMAIYDY4AvAUgaA3wNOA7YEngqyd8mklI8Hvhrxvvz+O/aFy/n+4FvAZvjZXe3pC1T3F8D01MevomXU3Pcgrdad8N/ix+b2UrgSGBRc7VzSf+W8nZByusk4Pd5SrMt71BrZPO/LvmNAnbB34lT03EgMATYFPhJXhrZ+FVNKILyk2sVHArMBl5vJf5a4EozW2tmk4B3gY+1EP8yM1tjZk/gH4MTAczscTObaWb1ZjYD/5OOaiGd04Hvm9lUc+aY2WuZ8GvMbFGqnf4e2LOFtBaZ2bVmts7MCv0Q32xmf0/xf9NC+r8GjpGU62I7iYaP61pcAexoZh+Y2XQzW17g85skpfFMyst84KesX47fM7NlZvYP4GoaFNXZwP+Y2WwzWwd8B29ZFdoquEbS28ALwGLgomyYmS1I5fUFYJKZTUq/92RgGt5SGQTsTcN78iT++62HfAziSOBsM3srvYNPFCjr54D7zWyyma0Ffgj0Aj6RJ3Oh79A+qWWRO17NC8/mP8flZrYy+Z0MXGVmc83sXWACMFaNu4Gy8auaUATl5xb8Y3Uqed1CzbA0fTRyrMJrM03xVqqt5XgN2BZAUp2kx1K3yTv4R6nJ7pbEQCD/z5blnwXKBLCghbANSt/M5uAK9dNJGRyDKwfwsn4QuD11bXxfUrd2yPIhkv5N0h8k/VPScvxjnl+O2fx++BsA2wP/m/uYAcsA4S24QjjfzPqa2QAzO9nMljTzzO2Bz2Y/nMD+wDZJlqbek6YYCCwzs7cKlC/Lttl0zaw+yZjNa1veoWdS3nPHR/PCm3rHsn6N5EnujfDWU0tpVCWhCMpMqlXPA44C7ung5Pul/tscg4Bck/3XwH3AQDPbDLge/wgBNGWSdgGQ/2drL/npr6TxIPlHNjD9XPfQGODFpBxINdgrzGxXvCY6msZjNO3h/wEvATuZWR+8q0d5cQZm3NnfYAFwVt4HrZeZ/WUDZYLGZbwAuCXvOZuY2XfxlkRT70lTLAA2l9S3lec1xSJcIQEgSXi5tNYCbi9NyZP1ayQPnud1wL9aSaMqCUVQGZwGHJRXK+sorpDUXdIn8Q/fncm/N167e0/SSLxVkmMJUI/3nea4EfiqpL3k7Niegc1meB74lHzO+mZ4M31DuB04DDiHhtYAkg6U9PE0HrMc7yqqb0O6PeQDzrmjC16Oy4F3Je2cnpnPf0rqJ2kg8BXgjuR/PTBB0m5Jvs3SWExHMxFvIR0uqWuS/QBJ26WKyDQa3pP9gU83lYiZLcYHhf8v5aebpNx40r+ALXIDrk3wG+BoSQenVth/AGuAjlB67eE24EL5QPmmeEvujrzWds0QiqACMLNXzWxaEZL+J/AWXvu5Fe/bfSmFfRm4UtIK4Bv4HzUnzyp8EPfPqSthHzO7M/n9GlgB3IsPPG4wqc/6DmAGPmj5hw1MbzHwNF7rvyMT9BHgLvzDPRt4Au8uyi3Gu76VpN/FB5dzx0H4wOtJeJn8LO95OX6H5+t5fJzm50nO3wLfw7uqlgN/w/vgOxQzW4C3jr6GK/kFwH/S8P8/CR/AXwb8Ny13UX4RV6AvAW/gg7+k9+o2YG56ZxrN6Tezl/GximuBN3Fl82kze7+d2dpX668j2LsN99+E//ZP4i3y94Dz2ilLp0cWG9MEQRDUNNEiCIIgqHFaVASpL3hgS3GCIAiCzk2LisC832hSiWQJgiAIykAhXUPPtXEQJgiCIOhEtDpYLOklYEd8wcVKfI60mdnQ4osXBEEQFJtCFEGTc8XzzAuUjP79+9vgwYPL8eggCIJOy/Tp0980sy2bCmvVvKqZvSZpD+CTyespM3uhIwVsC4MHD2batGJMuQ+CIKheJDVbeW91jEDSV/DFSFulY6Kkml14EQRBUG0UsuHCaUBdzvyBpO/hqzavLaZgQRAEQWkoZNaQcJvrOT5gfaNalc+MGXDOObB0aetxgyAIaohCFMHNwLPy3YYuxzdT+XlRpSoGS5fC9dfDlCnlliQIgqCiaG1lcRf8wz8eN0i1DBhvZlcXX7QOZq+9QApFEARBkEeLYwRmVi/pOjMbBjxXIpmKQ58+sOuu8Oyz5ZYkCIKgoiika+gRScenjSQ6N3V13iIIi6tBEAQfUogiOAvfzGSNpOWSViTb6Z2Pujro3RuWLGk9bhAEQY1QyBjBEWbWxcy6m1kfM+udtuTrfJxxBsybB1ttVW5JgiAIKobWrI/WAz8pkSzFpwp6t4IgCDqa2hojAPjGN+C448otRRAEQcVQW2MEAKtWwaRJ8H57t0oNgiCoLlpVBGlMoDrGCMAHjNes8ZXGQRAEQfOKQNIXMu798sLOLaZQRWXkSD/HeoIgCAKg5RbBRRl3voG5LxVBltIwaBBsvXWsMA6CIEi0tLJYzbibuu48SHDKKdC3b7klCYIgqAhaUgTWjLup687F979fbgmCIAgqhpYUwc6SZuC1/48mN+l6SNElKzbr1vmg8SablFuSIAiCstKSItilZFKUmlWrfHXxJZfA179ebmmCIAjKSrOKoFyb05eEjTeGgQNj5lAQBAGFLSirTsISaRAEAVDriuCNN+C16m34BEEQFELtKoLcwrJYTxAEQY3T4g5l8OGq4suB7VN8AWZmnXvm0NChcOWV8PGPl1uSIAiCstKqIsA3qr8QmA58UFxxSki3bnDZZeWWIgiCoOwU0jX0jpn90czeMLOluaPokpWC5cth8mRYu7bckgRBEJSNQhTBY5J+IGlfScNzR9ElKwWTJsFhh8HMmeWWJAiCoGwU0jVUl84jMn4GHNTx4pSYupS1KVNgeHXotiAIgrbSqiIwswNLIUhZGDwY+vf3hWVnn11uaYIgCMpCq11DkjaTdJWkaen4kaTNSiFc0ZEaFpYFQRDUKIWMEdwErABOTMdy4OZiClVS6upg9mwfOA6CIKhBChkj+KiZHZ+5vkLS80WSp/SMGwejR4cV0iAIapZCWgSrJe2fu0gLzFYXT6QSM2gQDBsGXbuWW5IgCIKyUEiL4Bzgl2lcQMAy4NRiClVy7r8fFi2CM84otyRBEAQlp5BZQ88De0jqk66rrzP99tvh4Yfh9NN9ADkIgqCGaFYRSPqCmU2UdFGePwBmdlWRZSsddXUwcSIsXOj7FARBENQQLbUIcqOnvZsIqy4j/rmFZc8+G4ogCIKao6Udyn6anA+b2Z+zYWnAuHoYOhS6d/f1BCecUG5pgiAISkohs4auLdCv89Kjh88ceuWVcksSBEFQcloaI9gX+ASwZd44QR+g+uZaTp4MvZvqBQuCIKhuWmoRdAc2xZVF78yxHCio/0TSEZJeljRH0iVNhJ8qaYmk59Nxetuz0EGEEgiCoEZpaYzgCeAJSb8wszZv7CupK3AdcCiwEJgq6T4zezEv6h1mdm5b0+9w3n3X1xEceyyceGK5pQmCICgZhYwR3Cipb+5CUj9JDxZw30hgjpnNNbP3gduBMe0TswRssgk89JAfQRAENUQhiqC/mb2duzCzt4CtCrhvALAgc70w+eVzvKQZku6SVL65m5JvaP/ss2UTIQiCoBwUogjqJQ3KXUjano5bR/B7YLCZDQUmA79sKpKkM3NmsJcsWdJBj26CujqYNQtWrCjeM4IgCCqMQhTBpcCfJN0iaSLwJDChgPteB7I1/O2S34ek/Y/XpMsbgb2aSsjMbjCzEWY2Yssttyzg0e2krg7MYPr04j0jCIKgwijE1tADaY/ifZLXBWb2ZgFpTwV2krQDrgDGAidlI0jaxswWp8tjgNkFS14MRo6EPfaANWtajxsEQVAltLSOYGczeymzUf2idB4kaZCZPddSwma2TtK5wIP4uoObzGyWpCuBaWZ2H3C+pGOAdVSCVdMttoDnny+rCEEQBKVGZk1390v6mZmdIemxJoLNzMqyef2IESNs2rRpxX1IfT10KaTXLAiCoHMgabqZjWgqrKV1BGekc/VuXt8Ud98N48f79pUDmprkFARBUF201DV0XEs3mtk9HS9OBTBggM8amjLFF5cFQRBUOS0NFn86nbfCbQ49mq4PBP4CVKci2HNP6NbN1xOEIgiCoAZoqWtoPICkh4Bdc7N7JG0D/KIk0pWDnj195tCUKeWWJAiCoCQUMiI6MDPFE+BfwKDmIlcFdXUwdSp88EG5JQmCICg6hWxe/0iyLXRbuv4c8HDxRKoAjj0W+vWD995zG0RBEARVTCELys6VdCzwqeR1g5n9trhilZmDD/YjCIKgBiikRQDwHLDCzB6WtLGk3mZW3QZ53n0XFi+GnXYqtyRBEARFpdUxAklnAHcBuT2MBwD3FlGmyuD44+Gzny23FEEQBEWnkMHifwf2w3cmw8xeoTAz1J2bkSNh5kxYubLckgRBEBSVQhTBmrSxDACSNqLjzFBXLnV1bmriuRZNKgVBEHR6ClEET0j6GtBL0qHAnfg+AtXNyJF+jo1qgiCocgpRBBcDS4CZwFnAJODrxRSqIthqKxg8OBaWBUFQ9bQ4ayhtQD/LzHYGflYakSqI//s/2GabcksRBEFQVFpUBGb2gaSX0/4D/yiVUBXDkUeWW4IgCIKiU8g6gn7ALElTgA+n0JjZMUWTqlJ47z34/e9hl11g993LLU0QBEFRKEQRXFZ0KSoVMzjpJPiv/4Jvf7vc0gRBEBSFlvYj6AmcDeyIDxT/3MzWlUqwiqBXLxg6NGYOBUFQ1bQ0a+iXwAhcCRwJ/KgkElUaI0e6JdL6+nJLEgRBUBRaUgS7mtkXzOynwAnAJ0skU2VRVwfLl8PLL5dbkiAIgqLQkiJYm3PUXJdQlro6P0+bVl45giAIikRLg8V7SFqe3MJXFi9PbjOzPkWXrhL42Mfg1Vdhhx3KLUll8cEHcOGFMGIEnHJKuaUJgmADaGmryq6lFKRi6dIFhgwptxSVx6WXwrXXuru+Hk49taziBEHQfgoxMRE88wyMGwerV5dbksrg7rvhe9+D006DQw+FiRNjMD0IOjGFbkxT2/zrX/CrX8GZZ8J++5VbmvKz227eHXTddQ37Onfp4usupPLKFgRBm4kWQSHkBoxr3QDde+/5x37nneGXv4QePWDjjf145x3f3nPy5HJLGQRBGwlFUAgf+QgMGlTbC8vq6+HEE+H0010ZNBW+bBmMGQNPPll6+YIgaDfNKgJJKyQtT8eKzPWKzGyi2mHkyNpuEXzrW253adiwprt/+vWDhx6C7beHo4+u7bIKgk5Gs4rAzHqbWZ909M5c966ZqaNZ9t3XTU6sWlVuSUrP/ffD5Zf7uMC//3vz8bbaCh5+2M+HHw4vvFAyEYMgaD8FdQ1J2l/S+OTuL6n2JtVfeCHMmuX94bXEK6/AySd7S+D661sfDB4wAB55xAeUa62sgqCT0uqsIUn/jdsc+hhwM9AdmIhvaF871OpsmIULYcst4Z57vEVUCIMHw1NPeZmZwZtvehpBEFQkhbQIjgWOIe1FYGaLgN7FFKpiOf98GDu23FKUlgMPhNmzve+/LeQU5yWX+PjKggUdL1sQBB1CIYrgfTMzwAAkbVJckSqYtWvht7+Fb3wDVqwotzTF5Uc/gh/8wGv0G23AcpMTT/TZRIcc4usxgiCoOApRBL+R9FOgr6QzgIepxf2LwQdMP/MZ+OY34aMfdRML66rQHt/kyb4Zz9SpG57WXnvBpEnexXTIIbB06YanGQRBhyJrak54fiTpUOCwdPmQmZVt1dCIESNsWrktgU6dChdf7H3ff/0rdK0is0zz5/vHe5tt3LTGppt2TLqPPOLTSocPhz/9yVciB0FQMiRNN7MRTYUV2uafCfTCu4dmdpRgnZa99/YP21tvuRJ4+23vArn4Yl9d21lZvRqOO87NRvz2tx2nBMDL5e67fewglEAQVBSt/iMlnQ5MAY7DN6h5RtKXii1YxSPB5pu7e+5c+Pvfvevj8MO9ldAZeewxmDkTbr0Vdtqp49M/+mg46ih3T54cRvyCoEIopGr2n8AwMzvVzMYBewEXF1esTsbw4b6D2VVX+QY2w4fDF74A779fbsnaxlFHwZw5/sEuJnPnwpFHwgkndL4yCoIqpBBFsBTITpFZkfyCLD16+KKzuXNhwgRYswa6d/ew995rf7orV7q5hhtv9Omro0bBz9JY/Zo1bvxt0aINk/1Pf4IHHnB3W6eJtochQ9xy6aRJcNJJno8gCMpGs2MEki5KzjnAs5J+h48RjAFmlEC2zslmm8F3vtNgmG3uXLdeesEFfmzSzOzb+np47TWYMcPjHHKIfyD79fNpq+B99h//uCsdcCN4uQ1hdt3V9wY49FA44IDmn5PP66/D8cd7N9fBB0O3bu3Ld1s56yw313HRRT4ecf758OMfe7ldcYUb+dthB1+ctt12pZMrCGqQZmcNpRXFzWJmV7SauHQE8L9AV+BGM/tuXngP4Fd4d9NS4HNmNr+lNCti1lBbmDfPWwq/+51bMb38cvjiFxvML0yY4NY6Z85sWJtw2GHw4IPuvvZaGDgQhg71j2J2oLW+3hXH5Ml+PPWUtz6eegr239+7eZYt81lATc1sWrPGWxizZrlS2XXXYpZE00ya5M8eNsyn5r75Jmy9deONbrp2he9+F776VTd3ffXVXhaDB7uyGDCgumZuBUERaGnWUEHTR9v50K7A34FDgYXAVODzZvZiJs6XgaFmdrakscCxZva5ltLtdIogx5//7HPz//IX3wf5pZfc//jj/eM3dGjDsdtu7Zuxs3q1P2fUKK9BX3SR17L79fPafq7FkNt/+ayz4IYbfDbPccd1XF43lLVrfSXy/Pl+zJvnLaRRo9yQ3bBhjU1hd+niA9xjx7oSPOyw9dO8804YPRr++Mem8/rAA57+Aw+4cu7TB3r39nOfPr5CevBgX2X9zDMN/rk4O+7oXYGrVrlCX7u24Vi3zn/zrl291bdgQYN/7jx6tOdj+nRX4N26+UK+bt083dxstHnzfLZafvigQR6+cqXP+sqaRJEa3qdVqxo2E8qWX64FuXKlK2GpIf0uXUprYqW+vqFczLwFHC3CDWaDpo9K2hL4L2A3oGfO38wOauXWkcAcM5ub0rkd71Z6MRNnDHB5ct8F/ESSrFjaqZzst5/3xd9/vw8s57j77o57Rq9e/sHMMWGCm3d46CFvMdx1F/Tv7yt8H3/clcCECZWlBMD/9EOGNL1X9B57uMLLKorXXvMPLXjr4Pzz178vl9bgwU2Hb7edn3v18o/q8uWweLH/VitWwHnnefijj8K5565//yuvuDK4+mrfzzmfJUu87H/6U/if/1k/fPVq6NkTbr7Zx0/yyyM3qH7FFT4ulGXzzRsW6o0bt/47tf32Xk7gra78zYN2391bpOAVhaefbhy+zz4Nfp/4hJdJt24NymjUKPjFLzx8zBh44w33l/xjPmpUQ5733ttlzSnBtWvh859v2P+6e/eGrtAc550H11zjLdg+fRoUVO75F1zgU7ffegs++cn1w88+28eiFi1yA4r5fOUrXi5z5sAZZ6wffsklPhtw5sym350rr/TnTpnicuTzwx96q/zxx/33y+e667w1PmmSr+bP56abvPK2bFnDTMUOppB1BLcCdwCjgbOBccCSAu4bAGQNzCwE6pqLY2brJL0DbAG8mY0k6UzgTIBBuZpPZ0Tymt/o0aV53pZbei157FivWb38stcou3RxG0J33eV/gM5Gjx7+0d1xx/XDhgzx/ZSbY5ddWg4fNcqP5hg/3mdVLV/ux4oVft52Ww8//HBvgeV/jHI18vHj4aCDGsKztXqAyy6DL3+5cYshW4O/4AI49tjGrYlst9i4cW4yPUufjNX4M890GbP079/gPu88b6XW13va69Y15A3gmGN8lXhWvl12aQjv29e7J9eu9XeuZ8/G41V77unh2fLZZ5+G8EsvbdwaAW8BgvtfdFHjltbatQ2VAMnd2bLJyZGjqb21WwvPxmvp/g0Nb+35RaTVrqHUnNhL0gwzG5r8pprZ3q3cdwJwhJmdnq6/CNSZ2bmZOH9LcRam61dTnDebShM6cddQEARBGWmpa6iQ6aO5dtpiSUdLGgYU0j55HRiYud4u+TUZR9JGwGbE1NQgCIKSUkjX0LckbQb8B3At0Ae4oID7pgI7pU1sXgfGAiflxbkP72p6Gl+1/GhVjg8EQRBUMK0qAjP7Q3K+AxwIIOmCAu5bJ+lc4EF8+uhNZjZL0pXANDO7D/g5cIukOcAyXFkEQRAEJaRd00cl/cPMyjJqK2kJ8Fo7b+9P3kB0jRPl0ZgojwaiLBpTDeWxvZk1uVVge3ccKdu+jc1lpBAkTWtusKQWifJoTJRHA1EWjan28mivPeDoxw+CIKgSWrI1tIKmP/jC9yYIgiAIqoBmFYGZVeMG9TeUW4AKI8qjMVEeDURZNKaqy6NotoaCIAiCzkHsGRgEQVDj1IwikHSEpJclzZF0SbnlKQaSBkp6TNKLkmZJ+kry31zSZEmvpHO/5C9J16QymSFpeCatcSn+K5LGlStPHYGkrpL+KukP6XoHSc+mfN8hqXvy75Gu56TwwZk0JiT/lyUd3syjKhpJfSXdJeklSbMl7VvL74akC9P/5G+SbpPUs1bfDcys6g98QdurwBCgO/ACsGu55SpCPrcBhid3b9wM+K7A94FLkv8lwPeS+yjgj/gEgH2AZ5P/5sDcdO6X3P3Knb8NKJeLgF8Df0jXvwHGJvf1wDnJ/WXg+uQeC9yR3Lumd6YHsEN6l7qWO1/tKIdfAqcnd3egb62+G7jBy3lAr8w7cWqtvhu10iL40CS2mb0P5ExiVxVmttjMnkvuFcBs/IUfg38ESOfPJPcY4FfmPAP0lbQNcDgw2cyWmdlbwGTgiNLlpOOQtB1wNHBjuhZwEG72HNYvj1w53QUcnOKPAW43szVmNg/ftW9kSTLQQSQzMZ/CV/NjZu+b2dvU8LuBT5bpleycbQwspgbfDaidrqGmTGIPKJMsJSE1XYcBzwJbm9niFPRPYOvkbq5cqqm8rsb308jZ990CeNvM1qXrbN4amUXHzapsQXWUxw64+fibUzfZjZI2oUbfDTN7Hfgh8A9cAbwDTKc2342aUQQ1haRNgbuBC8xseTbMvD1bE1PFJI0G3jCz6eWWpQLYCBgO/D8zGwasxLuCPqTG3o1+eG1+B2BbYBM6b8tmg6kVRVCISeyqQFI3XAncamb3JO9/pWY96fxG8m+uXKqlvPYDjpE0H+8OPAjfQ7tv6g6Axnlrzix6NZTHQmChmT2bru/CFUOtvhuHAPPMbImZrQXuwd+XWnw3akYRfGgSO80CGIubwK4qUp/lz4HZZnZVJihn7pt0/l3G/5Q0Q2Qf4J3UTfAgcJikfqnmdFjy61SY2QQz287MBuO/+aNmdjLwGG72HNYvj1w5Zc2i3weMTTNHdgB2AqaUKBsdgpn9E1ggKW3nxcH4trE1+W7gXUL7SNo4/W9y5VFz7wZQG7OG/PfiKHwWzavApeWWp0h53B9v2s8Ank/HUXhf5iPAK8DDwOYpvoDrUpnMBEZk0voSPvA1Bxhf7rx1QNkcQMOsoSH4n3UOcCfQI/n3TNdzUviQzP2XpnJ6GTiy3PlpZxnsCUxL78e9+Kyfmn03gCuAl4C/AbfgM39q8t2IlcVBEAQ1Tq10DQVBEATNEIogCIKgxglFEARBUOOEIgiCIKhxQhEEQRDUOKEIgopFkkmamLneSNKSjBXRY9SKJVlJ20q6K7lPlfSTNsrwtQLi/ELSCa3FKxaSHpdUtfvpBsUnFEFQyawEdpeU2xr1UDKrNs3sPjP7bksJmNkiM9uQj3SriqAzk1lFG9QwoQiCSmcSbj0U4PPAbbmAbA0/1cqvkfQXSXNzNXRJgyX9LZPewFSDfkXSf2fSulfS9GSf/szk913cOuXzkm5Nfqck+/wvSLolk+6n8p+dJckxW9LP0jMeyim4bI1eUv9kEiOXv3vl+wTMl3SupIuS0bhnJG2eecQXk5x/kzQy3b+JpJskTUn3jMmke5+kR/HFZEGNE4ogqHRux5fw9wSG4tZUm2MbfHX1aKC5lsJI4PiU1mczXSpfMrO9gBHA+ZK2MLNLgNVmtqeZnSxpN+DrwEFmtgfwlTY+eyfgOjPbDXg7ydEauwPHAXsD3wZWmRuNexo4JRNvYzPbE7ebf1PyuxQ3hTASOBD4QbI4Cm5n6AQzG1WADEGVE4ogqGjMbAYwGG8NTGol+r1mVm9mL9JgTjmfyWa21MxW44bG9k/+50t6AXgGNyK2UxP3HgTcaWZvJtmWtfHZ88zs+eSenvLVGo+Z2QozW4KbPv598p+Zd/9tSaYngT6S+uJ2gC6R9DzwOG4mYVCKPzlP/qCGif7BoDNwH247/gDcNk5zrMm41UycfJsqJukA3Brlvma2StLj+EezLRTy7GycD4Dc2Mc6Gipl+c/N3lOfua6n8f93vXwlOY43s5ezAZLq8PGXIACiRRB0Dm4CrjCzmR2Q1qHyfXp74btP/Rk3KfxWUgI741sz5libTHsDPIp3J20Bvhd0B8gDMB/YK7nbO7D9OQBJ++OWQt/BrYKel6xrImnYBsoZVCmhCIKKx8wWmtk1HZTcFHy/hhnA3WY2DXgA2EjSbLx//5lM/BuAGZJuNbNZeD/9E6kb6So6hh8C50j6K9C/nWm8l+6/Hjgt+X0T6IbLPytdB8F6hPXRIAiCGidaBEEQBDVOKIIgCIIaJxRBEARBjROKIAiCoMYJRRAEQVDjhCIIgiCocUIRBEEQ1DihCIIgCGqc/w/Hc4sLAiHsygAAAABJRU5ErkJggg==\\n\",\n","      \"text/plain\": [\n","       \"<Figure size 432x288 with 1 Axes>\"\n","      ]\n","     },\n","     \"metadata\": {\n","      \"needs_background\": \"light\"\n","     },\n","     \"output_type\": \"display_data\"\n","    }\n","   ],\n","   \"source\": [\n","    \"# Compute the moving average loss to smooth out the noise in SGD\\n\",\n","    \"plotdata[\\\"avgloss\\\"] = moving_average(plotdata[\\\"loss\\\"])\\n\",\n","    \"plotdata[\\\"avgerror\\\"] = moving_average(plotdata[\\\"error\\\"])\\n\",\n","    \"\\n\",\n","    \"# Plot the training loss and the training error\\n\",\n","    \"import matplotlib.pyplot as plt\\n\",\n","    \"\\n\",\n","    \"plt.figure(1)\\n\",\n","    \"plt.subplot(211)\\n\",\n","    \"plt.plot(plotdata[\\\"batchsize\\\"], plotdata[\\\"avgloss\\\"], 'b--')\\n\",\n","    \"plt.xlabel('Minibatch number')\\n\",\n","    \"plt.ylabel('Loss')\\n\",\n","    \"plt.title('Minibatch run vs. Training loss')\\n\",\n","    \"\\n\",\n","    \"plt.show()\\n\",\n","    \"\\n\",\n","    \"plt.subplot(212)\\n\",\n","    \"plt.plot(plotdata[\\\"batchsize\\\"], plotdata[\\\"avgerror\\\"], 'r--')\\n\",\n","    \"plt.xlabel('Minibatch number')\\n\",\n","    \"plt.ylabel('Label Prediction Error')\\n\",\n","    \"plt.title('Minibatch run vs. Label Prediction Error')\\n\",\n","    \"plt.show()\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"EFlJYAUniISD\"\n","   },\n","   \"source\": [\n","    \"## Evaluation / Testing \\n\",\n","    \"\\n\",\n","    \"Now that we have trained the network, let us evaluate the trained network on the test data. This is done using `trainer.test_minibatch`.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 20,\n","   \"metadata\": {\n","    \"id\": \"IDPyQqogiISD\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"Average test error: 1.79%\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"# Read the training data\\n\",\n","    \"reader_test = create_reader(test_file, False, input_dim, num_output_classes)\\n\",\n","    \"\\n\",\n","    \"test_input_map = {\\n\",\n","    \"    label  : reader_test.streams.labels,\\n\",\n","    \"    input  : reader_test.streams.features,\\n\",\n","    \"}\\n\",\n","    \"\\n\",\n","    \"# Test data for trained model\\n\",\n","    \"test_minibatch_size = 512\\n\",\n","    \"num_samples = 10000\\n\",\n","    \"num_minibatches_to_test = num_samples // test_minibatch_size\\n\",\n","    \"test_result = 0.0\\n\",\n","    \"\\n\",\n","    \"for i in range(num_minibatches_to_test):\\n\",\n","    \"    \\n\",\n","    \"    # We are loading test data in batches specified by test_minibatch_size\\n\",\n","    \"    # Each data point in the minibatch is a MNIST digit image of 784 dimensions \\n\",\n","    \"    # with one pixel per dimension that we will encode / decode with the \\n\",\n","    \"    # trained model.\\n\",\n","    \"    data = reader_test.next_minibatch(test_minibatch_size,\\n\",\n","    \"                                      input_map = test_input_map)\\n\",\n","    \"\\n\",\n","    \"    eval_error = trainer.test_minibatch(data)\\n\",\n","    \"    test_result = test_result + eval_error\\n\",\n","    \"\\n\",\n","    \"# Average of evaluation errors of all test minibatches\\n\",\n","    \"print(\\\"Average test error: {0:.2f}%\\\".format(test_result*100 / num_minibatches_to_test))\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"HZKHdiRXiISE\"\n","   },\n","   \"source\": [\n","    \"Note, this error is very comparable to our training error indicating that our model has good \\\"out of sample\\\" error a.k.a. generalization error. This implies that our model can very effectively deal with previously unseen observations (during the training process). This is key to avoid the phenomenon of overfitting.\\n\",\n","    \"\\n\",\n","    \"This is a **huge** reduction in error compared to multi-class LR (from Lab 02).\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"ONKnAXgOiISE\"\n","   },\n","   \"source\": [\n","    \"We have so far been dealing with aggregate measures of error. Let us now get the probabilities associated with individual data points. For each observation, the `eval` function returns the probability distribution across all the classes. The classifier is trained to recognize digits, hence has 10 classes. First let us route the network output through a `softmax` function. This maps the aggregated activations across the network to probabilities across the 10 classes.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 21,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"mo7o3EjciISE\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"out = C.softmax(z)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"3ELV_ptliISE\"\n","   },\n","   \"source\": [\n","    \"Let us test a small minibatch sample from the test data.\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 22,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"e2xN-1mDiISF\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Read the data for evaluation\\n\",\n","    \"reader_eval = create_reader(test_file, False, input_dim, num_output_classes)\\n\",\n","    \"\\n\",\n","    \"eval_minibatch_size = 25\\n\",\n","    \"eval_input_map = {input: reader_eval.streams.features} \\n\",\n","    \"\\n\",\n","    \"data = reader_test.next_minibatch(eval_minibatch_size, input_map = test_input_map)\\n\",\n","    \"\\n\",\n","    \"img_label = data[label].asarray()\\n\",\n","    \"img_data = data[input].asarray()\\n\",\n","    \"predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 23,\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"ze-HQR3yiISF\"\n","   },\n","   \"outputs\": [],\n","   \"source\": [\n","    \"# Find the index with the maximum value for both predicted as well as the ground truth\\n\",\n","    \"pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\\n\",\n","    \"gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 24,\n","   \"metadata\": {\n","    \"id\": \"n2mCxD11iISF\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"Label    : [4, 5, 6, 7, 8, 9, 7, 4, 6, 1, 4, 0, 9, 9, 3, 7, 8, 4, 7, 5, 8, 5, 3, 2, 2]\\n\",\n","      \"Predicted: [4, 6, 6, 7, 8, 9, 7, 4, 6, 1, 4, 0, 9, 9, 3, 7, 8, 6, 7, 5, 8, 6, 3, 2, 2]\\n\"\n","     ]\n","    }\n","   ],\n","   \"source\": [\n","    \"print(\\\"Label    :\\\", gtlabel[:25])\\n\",\n","    \"print(\\\"Predicted:\\\", pred)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"id\": \"oswD-Ob5iISF\"\n","   },\n","   \"source\": [\n","    \"As you can see above, our model is much better.  Do you see any mismatches?  \\n\",\n","    \"\\n\",\n","    \"Let us visualize one of the test images and its associated label.  Do they match?\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 25,\n","   \"metadata\": {\n","    \"id\": \"Koz6l8YmiISG\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"Image Label:  9\\n\"\n","     ]\n","    },\n","    {\n","     \"data\": {\n","      \"image/png\": \"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\\n\",\n","      \"text/plain\": [\n","       \"<Figure size 432x288 with 1 Axes>\"\n","      ]\n","     },\n","     \"metadata\": {\n","      \"needs_background\": \"light\"\n","     },\n","     \"output_type\": \"display_data\"\n","    }\n","   ],\n","   \"source\": [\n","    \"# Plot a random image\\n\",\n","    \"sample_number = 5\\n\",\n","    \"plt.imshow(img_data[sample_number].reshape(28,28), cmap=\\\"gray_r\\\")\\n\",\n","    \"plt.axis('off')\\n\",\n","    \"\\n\",\n","    \"img_gt, img_pred = gtlabel[sample_number], pred[sample_number]\\n\",\n","    \"print(\\\"Image Label: \\\", img_pred)\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"markdown\",\n","   \"metadata\": {\n","    \"collapsed\": true,\n","    \"id\": \"Vn_6hmzkiISG\"\n","   },\n","   \"source\": [\n","    \"**Suggested Explorations**\\n\",\n","    \"-  Try exploring how the classifier behaves with different parameters - suggest changing the `minibatch_size` parameter from 25 to say 64 or 128. What happens to the error rate? How does the error compare to the logistic regression classifier?\\n\",\n","    \"- Try increasing the number of sweeps\\n\",\n","    \"- Can you change the network to reduce the training error rate? When do you see *overfitting* happening?\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 26,\n","   \"metadata\": {\n","    \"id\": \"I5B5-7TLhhbF\"\n","   },\n","   \"outputs\": [\n","    {\n","     \"name\": \"stderr\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"/usr/local/envs/test36/lib/python3.6/site-packages/cntk/core.py:361: UserWarning: your data is of type \\\"int64\\\", but your input variable (uid \\\"Input3\\\") expects \\\"<class 'numpy.float32'>\\\". Please convert your data beforehand to speed up training.\\n\",\n","      \"  (sample.dtype, var.uid, str(var.dtype)))\\n\"\n","     ]\n","    },\n","    {\n","     \"data\": {\n","      \"text/plain\": [\n","       \"8\"\n","      ]\n","     },\n","     \"execution_count\": 26,\n","     \"metadata\": {},\n","     \"output_type\": \"execute_result\"\n","    }\n","   ],\n","   \"source\": [\n","    \"from PIL import Image as Image1\\n\",\n","    \"img = Image1.open(\\\"MysteryNumberD.bmp\\\")\\n\",\n","    \"Pix=list(img.getdata())\\n\",\n","    \"Ia=np.asarray(Pix)\\n\",\n","    \"np.argmax(out.eval(Ia))\"\n","   ]\n","  }\n"," ],\n"," \"metadata\": {\n","  \"anaconda-cloud\": {},\n","  \"colab\": {\n","   \"collapsed_sections\": [],\n","   \"name\": \"IU6_Lab3_MultiLayerPerceptron.ipynb\",\n","   \"provenance\": []\n","  },\n","  \"kernelspec\": {\n","   \"display_name\": \"Python 3\",\n","   \"language\": \"python\",\n","   \"name\": \"python3\"\n","  },\n","  \"language_info\": {\n","   \"codemirror_mode\": {\n","    \"name\": \"ipython\",\n","    \"version\": 3\n","   },\n","   \"file_extension\": \".py\",\n","   \"mimetype\": \"text/x-python\",\n","   \"name\": \"python\",\n","   \"nbconvert_exporter\": \"python\",\n","   \"pygments_lexer\": \"ipython3\",\n","   \"version\": \"3.6.13\"\n","  }\n"," },\n"," \"nbformat\": 4,\n"," \"nbformat_minor\": 0\n","}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZflaWJpTLv0U"},"source":["**Force copy output jupyter files to persistent location**"]},{"cell_type":"code","metadata":{"id":"cVG5XcuuKvTc","executionInfo":{"status":"ok","timestamp":1622903499285,"user_tz":-480,"elapsed":6,"user":{"displayName":"Rajesh kumar Ganesan","photoUrl":"","userId":"04176200856727597046"}}},"source":["!yes | cp -rf /content/DLF_IU6_Lab3_MultiLayerPerceptron_output.ipynb /content/gdrive/MyDrive/PAI0221A/4.DLF/"],"execution_count":11,"outputs":[]}]}